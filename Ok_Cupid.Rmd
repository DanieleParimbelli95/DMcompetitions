---
title: "Ok Cupid"
output: html_document
---

NAME: Daniele Parimbelli

BADGE: 790212

NICKNAME: d.parimbelli2

TEAM: BoxCox

ROUND: 1st

### Summary

La strategia è stata

1. eliminare le variabili factor con un solo livello
2. trasformare alcune variabili numeriche che sono in realtà delle dummy
3. one hot encoding delle variabili factor
4. eliminare le variabili con varianza prossima allo zero
5. ridge
6. xgboost
7. media pesata tra le due previsioni

### References:

* Guida al pacchetto caret di Max Kuhn: http://topepo.github.io/caret/index.html

* Tuning per Xgboost: [XGBoost Documentation](https://xgboost.readthedocs.io/en/latest/index.html) e [Complete Guide to Parameter Tuning in XGBoost](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)

### Models

* Ridge
* Xgboost

### Non-standard R packages

-

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, comment=NA, cache=F, R.options=list(width=220))
```


### R code to reproduce the last submission:

```{r}

# pacchetti di R
library(caret)
library(glmnet)

# importazione dei dati
train <- read.csv("http://bee-fore.s3-eu-west-1.amazonaws.com/datasets/101.csv", stringsAsFactors = T)
test <- read.csv("http://bee-fore.s3-eu-west-1.amazonaws.com/datasets/102.csv",  stringsAsFactors = T)

n<-nrow(train)
m<-nrow(test)

test$Class<-NA

# combinazione train e test per il preprocessing dei dati
fulldata=rbind(train,test)

# eliminazione di variabili factor con un solo livello
fulldata<-fulldata[,-c(21:28)]

# trasformazione di variabili numeriche che sono in realtà delle dummy
fulldata[,c(17,21,24:83)] <- lapply(fulldata[,c(17,21,24:83)], factor)

# one hot encoding per le variabili categoriali (ad eccezione della risposta) con la funzione DummyVars di caret
dummies <- dummyVars(~., fulldata[,-23])
fulldata.with.dummies <- data.frame(predict(dummies, fulldata[,-23]))

# creazione del nuovo dataset comprensivo anche della risposta
fulldata.with.dummies<-cbind(fulldata.with.dummies,Class=fulldata[,23])

# individuazione delle variabili con varianza prossima allo zero (parametro freqCut scelto in modo da escludere le variabili con all'incirca un numero inferiore alle 10 osservazioni)
vars_zv = nearZeroVar(fulldata.with.dummies, freqCut = 1000/1, uniqueCut = 10)

# eliminazione delle variabili con varianza prossima allo zero
fulldata.with.dummies<-fulldata.with.dummies[,-vars_zv]

# separazione di train e test
train<-fulldata.with.dummies[1:n,]
test<-fulldata.with.dummies[(n+1):(n+m),]


# RIDGE

# metodo di training dei parametri
ctrl <- trainControl(method = "cv",
                     number = 10,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     sampling="down") # down sampling per trattare la class imbalance

set.seed(457) # per riproducibilità

# train model
fit.ridge <- train(Class ~ ., 
                   train,
                   method = "glmnet",
                   trControl=ctrl,
                   tuneGrid=expand.grid(alpha=0,lambda=seq(0.001, 1, by = 0.001)),
                   preProcess="nzv",  # eliminazione di tutte le variabili con varianza vicina allo zero
                   family="binomial",
                   metric = "ROC")

# previsioni ridge
phat_ridge=predict(fit.ridge, newdata=test,type="prob")[,2]


# XGBOOST

# la griglia usata per il tuning è messa come commento perché il software ci mette tantissimo tempo a fornire i risultati (più in basso è fornita la griglia effettivamente utilizzata)

# my_grid<-expand.grid(nrounds=c(200,300,400,500), eta=seq(0.01,0.15,by=0.01), max_depth=c(3,4,5,6), gamma=seq(0,1,by=0.2), min_child_weight=c(2,3,4,5), subsample=1, colsample_bytree=c(0.9,1))

# ctrl <- trainControl(method = "cv",
#                      number = 10,
#                      classProbs = TRUE,
#                      summaryFunction = twoClassSummary,
#                      sampling="down")

# set.seed(457)

# fit.xgb <- train(Class ~ ., 
#                  train,
#                  method = "xgbTree",
#                  trControl=ctrl,
#                  tuneGrid=my_grid,
#                  family="binomial",
#                  metric = "ROC")

# i risultati ottenuti sulla base dell'ottimizzazione della curva ROC hanno portato alla seguente griglia di parametri:

my_grid<-expand.grid(nrounds=300, eta=0.03, max_depth=5, gamma=0,
                     min_child_weight=4, subsample=1, colsample_bytree=0.9)

# caret model training parameters
ctrl <- trainControl(method = "cv",
                     number = 10,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     sampling="down") # down sampling per trattare la class imbalance

set.seed(457) # per riproducibilità

# train model
fit.xgb <- train(Class ~ ., 
                 train,
                 method = "xgbTree",
                 trControl=ctrl,
                 tuneGrid=my_grid,
                 family="binomial",
                 metric = "ROC")

# previsioni xgboost
phat_xgb=predict(fit.xgb, newdata=test,type="prob")[,2]

# media pesata tra le due previsioni, dando maggior peso alla previsione milgiore sulla base della classifica parziale
phat=phat_ridge*0.7+phat_xgb*0.3

# primi valori delle previsioni finali
head(phat)

```

